using CSV, DataFrames, JSON3
using Statistics, StatsBase
using Printf

# Define functions to compute correlations and boostrap CI intervals

"Returns the concordance correlation coefficient (CCC) for two vectors."
function ccc(x, y; corrected::Bool=true)
    @assert length(x) == length(y)
    s_xy = cov(x, y; corrected)
    var_x = var(x; corrected)
    var_y = var(y; corrected)
    m_x = mean(x)
    m_y = mean(y)
    return 2 * s_xy / (var_x + var_y + (m_x - m_y)^2)
end

"""
    sim_with(f::Function, x::AbstractMatrix)

Returns a function `sim_with_x(cols...)` that computes similarity of a 
set of vectors `cols` according to a function `f` with an input matrix `x`.
"""
function sim_with(f::Function, x::AbstractMatrix)
    function sim_with_x(cols...)
        y = hcat(cols...)
        return f(vec(y), vec(x))
    end
    return sim_with_x
end

"""
    sim_ci_with(f::Function, xs::Vector)

Returns a function `sim_ci_with_xs(cols...)` that computes a 95% confidence
interval for the similarity of a set of vectors `cols`, where similarity is 
computed according to a function `f` against each element `x` of a set 
of samples `xs`.
"""
function sim_ci_with(f::Function, xs::Vector)
    function sim_ci_with_xs(cols...)
        y = hcat(cols...)
        all_sims = map(x -> f(vec(y), vec(x)), xs)
        if any(ismissing.(all_sims))
            return [missing missing]
        elseif any(isnan.(all_sims))
            return [NaN NaN]
        else
            return quantile(all_sims, [0.025, 0.975])'
        end
    end
    return sim_ci_with_xs
end

"""
    sim_se_with(f::Function, xs::Vector)

Returns a function `sim_se_with_xs(cols...)` that computes the standard
error of the similarity of a set of vectors `cols`, where similarity is
computed according to a function `f` against each element `x` of a set
of samples `xs`.
"""
function sim_se_with(f::Function, xs::Vector)
    function sim_se_with_xs(cols...)
        y = hcat(cols...)
        all_sims = map(x -> f(vec(y), vec(x)), xs)
        if any(ismissing.(all_sims))
            return missing
        else
            return std(all_sims)
        end
    end
    return sim_se_with_xs
end

RESULTS_DIR = joinpath(@__DIR__, "results")
HUMAN_RESULTS_DIR = joinpath(RESULTS_DIR, "humans")
MODEL_RESULTS_DIR = joinpath(RESULTS_DIR, "models")
STIMULI_JSON_PATH = joinpath(@__DIR__, "dataset/stimuli/stimuli.json")

## Preprocess human data

# Load firebase data
firebase_path = joinpath(HUMAN_RESULTS_DIR, "firebase_data.json")
firebase_data = JSON3.read(read(firebase_path, String))

# Load Qualtrics data
qualtrics_path = joinpath(HUMAN_RESULTS_DIR, "qualtrics_data.csv")
qualtrics_data = CSV.read(qualtrics_path, DataFrame, header=2, skipto=4)

# Extract experiment completion codes generated by Firebase app
participant_codes = qualtrics_data[:, "Experiment-Code"]
participant_codes = [participant_codes; 1706036577920]

# Extract Firebase entries and entry names
entries = firebase_data.results
entry_names = collect(keys(firebase_data.results))

# Extract global participant data
participant_df = DataFrame()
for code in participant_codes
    row = Dict()
    entry = entries[Symbol(code)]
    row[:participant_code] = code
    # Add stimuli set
    row[:stimuli_set] = entry.stimuli_set
    # Add exam score
    row[:exam_results] = entry.exam.results
    row[:exam_score] = entry.exam.score
    # Add total reward
    row[:total_reward] = parse(Float64, entry.total_reward)
    # Add total payment
    row[:total_payment] = parse(Float64, entry.total_payment)
    push!(participant_df, row; cols=:union)
end

# Add Prolific IDs
prolific_id = qualtrics_data[:, "Prolific-ID"]
prolific_id = [prolific_id; "5ea237d872cd2a144b99a177"]
participant_df.prolific_id = prolific_id

# Print bonus for bulk payment on Prolific
for row in eachrow(participant_df)
    if row.total_payment <= 0.0 continue end
    println(row.prolific_id, ", ", round(row.total_payment, digits=2))
end

# Write participant data to CSV
CSV.write(joinpath(HUMAN_RESULTS_DIR, "participant_data.csv"), participant_df)

# Extract per-participant stimulus responses
stimuli_df = DataFrame(
    participant_code = Int[],
    plan_id = String[],
    step = Int[],
    timestep = Int[],
    goal_probs_1 = Float64[],
    goal_probs_2 = Float64[],
    goal_probs_3 = Float64[],
    goal_probs_4 = Float64[],
    true_goal_probs = Float64[],
    statement_ratings_1 = Float64[],
    statement_ratings_2 = Float64[],
    statement_probs_1 = Float64[],
    statement_probs_2 = Float64[],
    reward = Float64[],
    goal_reward = Float64[],
    time_spent = Float64[],
)
for code in participant_codes
    participant_entry = entries[Symbol(code)]
    for (name, stim_entry) in pairs(participant_entry)
        # Extract only entries of the form X_X
        m = match(r"(\d+)_(\d+)", string(name))
        isnothing(m) && continue
        for (step_idx, step_entry) in stim_entry
            step_idx == :reward && continue
            row = Dict{Symbol, Any}()
            for (key, val) in step_entry
                if val isa AbstractArray
                    for (idx, v) in enumerate(val)
                        row[Symbol(key, "_", idx)] = v
                    end
                elseif val isa Real
                    row[key] = val
                else
                    row[key] = parse(Float64, val)
                end
            end
            row[:participant_code] = code
            row[:plan_id] = name
            row[:step] = parse(Int, string(step_idx)) + 1
            push!(stimuli_df, row; cols=:union)
        end
    end
end
sort!(stimuli_df, [:participant_code, :plan_id, :timestep])

# Write per-participant data to CSV
CSV.write(joinpath(HUMAN_RESULTS_DIR, "per_participant_stimuli_data.csv"), stimuli_df)

# Summarize data by stimulus
group_df = DataFrames.groupby(stimuli_df, [:plan_id, :timestep])
stimuli_df = combine(group_df,
    ["goal_probs_$i" => mean => "goal_probs_$i" for i in 1:4]...,
    ["goal_probs_$i" => std => "goal_probs_std_$i" for i in 1:4]...,
    ["goal_probs_$i" => sem => "goal_probs_sem_$i" for i in 1:4]...,
    "true_goal_probs" => mean => "true_goal_probs",
    "true_goal_probs" => std => "true_goal_probs_std",
    ["statement_probs_$i" => mean => "statement_probs_$i" for i in 1:2]...,
    ["statement_probs_$i" => std => "statement_probs_std_$i" for i in 1:2]...,
    ["statement_probs_$i" => sem => "statement_probs_sem_$i" for i in 1:2]...,
    :goal_reward => mean => :goal_reward,
    :goal_reward => std => :goal_reward_std,
    :reward => mean => :reward,
    :reward => std => :reward_std,
)
sort!(stimuli_df, [:plan_id, :timestep])

# Write aggregated data per stimulus to CSV
CSV.write(joinpath(HUMAN_RESULTS_DIR, "stimuli_data.csv"), stimuli_df)

## Comparative analysis

NORMALIZE_STATEMENT_PRIOR = true

# Read human data
df_path = "stimuli_data.csv"
df_path = joinpath(HUMAN_RESULTS_DIR, df_path)
human_df = CSV.read(df_path, DataFrame)
human_df[!, :method] .= "human"
sort!(human_df, [:plan_id, :timestep])

# Decide whether to load data with or without normalized statement prior
prior = NORMALIZE_STATEMENT_PRIOR ? "_uniform" : ""

# Read data for belief-only model
df_path = "results_belief$prior.csv"
df_path = joinpath(MODEL_RESULTS_DIR, df_path)
belief_df = CSV.read(df_path, DataFrame)
belief_df[!, :method] .= "belief_only"
sort!(belief_df, [:plan_id, :timestep])

# Read data for goal-lesioned model
df_path = "results_goal$prior.csv"
df_path = joinpath(MODEL_RESULTS_DIR, df_path)
goal_df = CSV.read(df_path, DataFrame)
goal_df[!, :method] .= "goal_only"
sort!(goal_df, [:plan_id, :timestep])

# Read data for joint inference model
df_path = "results_joint$prior.csv"
df_path = joinpath(MODEL_RESULTS_DIR, df_path)
joint_df = CSV.read(df_path, DataFrame)
joint_df[!, :method] .= "joint"
joint_df.is_judgment .= joint_df.is_judgment
sort!(joint_df, [:plan_id, :timestep])

# Filter out non-judgment points
filter!(r -> r.is_judgment, belief_df)
filter!(r -> r.is_judgment, goal_df)
filter!(r -> r.is_judgment, joint_df)

combined_df = vcat(belief_df, goal_df, joint_df, human_df, cols=:union)

# Compute aggregate statistics across dataset
gdf = DataFrames.groupby(combined_df, [:method])
performance_df = combine(gdf,
    :true_goal_probs => mean => :true_goal_probs,
)
CSV.write(joinpath(RESULTS_DIR, "performance_results$prior.csv"), performance_df)

## Correlational analysis

get_goal_probs(df) = df[:, ["goal_probs_$i" for i in 1:4]] |> Matrix
get_statement_probs(df) = df[:, ["statement_probs_$i" for i in 1:2]] |> Matrix

# Extract mean human goal and assist probs
mean_human_goal_probs = get_goal_probs(human_df)
mean_human_statement_probs = get_statement_probs(human_df)

# Compute bootstrap samples of human goal probs
df_path = joinpath(HUMAN_RESULTS_DIR, "per_participant_stimuli_data.csv")
per_participant_df = CSV.read(df_path, DataFrame)
participant_ids = unique(per_participant_df.participant_code)

sampled_human_goal_probs = Vector{Matrix{Float64}}()
sampled_human_statement_probs = Vector{Matrix{Float64}}()
for _ in 1:1000
    sampled_ids = sample(participant_ids, length(participant_ids), replace=true)
    tmp_df = filter(row -> row.participant_code in sampled_ids, per_participant_df)
    tmp_gdf = DataFrames.groupby(tmp_df, [:plan_id, :timestep])
    tmp_mean_df = combine(tmp_gdf,
        ["goal_probs_$i" => mean => "goal_probs_$i" for i in 1:4]...,
        ["statement_probs_$i" => mean => "statement_probs_$i" for i in 1:2]...
    )
    if size(tmp_mean_df, 1) < size(human_df, 1)
        continue
    end
    sort!(tmp_mean_df, [:plan_id, :timestep])
    push!(sampled_human_goal_probs, get_goal_probs(tmp_mean_df))
    push!(sampled_human_statement_probs, get_statement_probs(tmp_mean_df))
end

# Compute correlation with each baseline
model_df = filter(r -> r.method != "human", combined_df)
gdf = DataFrames.groupby(model_df, [:method])

correlation_df = combine(gdf,
    ["goal_probs_$i" for i in 1:4] => sim_with(cor, mean_human_goal_probs) => :goal_cor,
    ["goal_probs_$i" for i in 1:4] => sim_se_with(cor, sampled_human_goal_probs) => :goal_cor_se,
    ["goal_probs_$i" for i in 1:4] => sim_ci_with(cor, sampled_human_goal_probs) => [:goal_cor_ci_lo, :goal_cor_ci_hi],
    ["statement_probs_$i" for i in 1:2] => sim_with(cor, mean_human_statement_probs) => :statement_cor,
    ["statement_probs_$i" for i in 1:2] => sim_se_with(cor, sampled_human_statement_probs) => :statement_cor_se,
    ["statement_probs_$i" for i in 1:2] => sim_ci_with(cor, sampled_human_statement_probs) => [:statement_cor_ci_lo, :statement_cor_ci_hi]
)
CSV.write(joinpath(RESULTS_DIR, "human_model_correlations$prior.csv"), correlation_df)

ccc_df = combine(gdf,
    ["goal_probs_$i" for i in 1:4] => sim_with(ccc, mean_human_goal_probs) => :goal_ccc,
    ["goal_probs_$i" for i in 1:4] => sim_se_with(ccc, sampled_human_goal_probs) => :goal_ccc_se,
    ["goal_probs_$i" for i in 1:4] => sim_ci_with(ccc, sampled_human_goal_probs) => [:goal_ccc_ci_lo, :goal_ccc_ci_hi],
    ["statement_probs_$i" for i in 1:2] => sim_with(ccc, mean_human_statement_probs) => :statement_ccc,
    ["statement_probs_$i" for i in 1:2] => sim_se_with(ccc, sampled_human_statement_probs) => :statement_ccc_se,
    ["statement_probs_$i" for i in 1:2] => sim_ci_with(ccc, sampled_human_statement_probs) => [:statement_ccc_ci_lo, :statement_ccc_ci_hi]
)
CSV.write(joinpath(RESULTS_DIR, "human_model_ccc$prior.csv"), ccc_df)

# Combine correlation results for both prior-corrected and non-prior corrected models

path = joinpath(RESULTS_DIR, "human_model_correlations.csv")
cor_df1 = CSV.read(path, DataFrame)
cor_df1[!, :normalize_prior] .= false

path = joinpath(RESULTS_DIR, "human_model_correlations_uniform.csv")
cor_df2 = CSV.read(path, DataFrame)
cor_df2[!, :normalize_prior] .= true

combined_cor_df = vcat(cor_df1, cor_df2, cols=:union)
combined_cor_df = combined_cor_df[:, ["method", names(combined_cor_df)[end], names(combined_cor_df)[2:end-1]...]]
CSV.write(joinpath(RESULTS_DIR, "human_model_correlations_all.csv"), combined_cor_df)

path = joinpath(RESULTS_DIR, "human_model_ccc.csv")
ccc_df1 = CSV.read(path, DataFrame)
ccc_df1[!, :normalize_prior] .= false

path = joinpath(RESULTS_DIR, "human_model_ccc_uniform.csv")
ccc_df2 = CSV.read(path, DataFrame)
ccc_df2[!, :normalize_prior] .= true

combined_ccc_df = vcat(ccc_df1, ccc_df2, cols=:union)
combined_ccc_df = combined_ccc_df[:, ["method", names(combined_ccc_df)[end], names(combined_ccc_df)[2:end-1]...]]
CSV.write(joinpath(RESULTS_DIR, "human_model_ccc_all.csv"), combined_ccc_df)
